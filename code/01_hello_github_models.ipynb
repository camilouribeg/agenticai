{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "745b8ee8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<svg aria-hidden=\"true\" focusable=\"false\" class=\"octicon octicon-mark-github pb-3\" viewBox=\"0 0 24 24\" width=\"64\" height=\"64\" fill=\"currentColor\" display=\"inline-block\" overflow=\"visible\" style=\"vertical-align: text-bottom;\">\n",
    "\n",
    "<path d=\"M12 1C5.9225 1 1 5.9225 1 12C1 16.8675 4.14875 20.9787 8.52125 22.4362C9.07125 22.5325 9.2775 22.2025 9.2775 21.9137C9.2775 21.6525 9.26375 20.7862 9.26375 19.865C6.5 20.3737 5.785 19.1912 5.565 18.5725C5.44125 18.2562 4.905 17.28 4.4375 17.0187C4.0525 16.8125 3.5025 16.3037 4.42375 16.29C5.29 16.2762 5.90875 17.0875 6.115 17.4175C7.105 19.0812 8.68625 18.6137 9.31875 18.325C9.415 17.61 9.70375 17.1287 10.02 16.8537C7.5725 16.5787 5.015 15.63 5.015 11.4225C5.015 10.2262 5.44125 9.23625 6.1425 8.46625C6.0325 8.19125 5.6475 7.06375 6.2525 5.55125C6.2525 5.55125 7.17375 5.2625 9.2775 6.67875C10.1575 6.43125 11.0925 6.3075 12.0275 6.3075C12.9625 6.3075 13.8975 6.43125 14.7775 6.67875C16.8813 5.24875 17.8025 5.55125 17.8025 5.55125C18.4075 7.06375 18.0225 8.19125 17.9125 8.46625C18.6138 9.23625 19.04 10.2125 19.04 11.4225C19.04 15.6437 16.4688 16.5787 14.0213 16.8537C14.42 17.1975 14.7638 17.8575 14.7638 18.8887C14.7638 20.36 14.75 21.5425 14.75 21.9137C14.75 22.2025 14.9563 22.5462 15.5063 22.4362C19.8513 20.9787 23 16.8537 23 12C23 5.9225 18.0775 1 12 1Z\">\n",
    "\n",
    "</path>\n",
    "\n",
    "</svg>\n",
    "\n",
    "\n",
    "## ****GitHub Marketplace Models****\n",
    "\n",
    "A catalog and playground of AI models to help you build AI features and products.\n",
    "\n",
    "* **Model switching**: A single API key for all models & billing.\n",
    "\n",
    "* **Quick personal setup**: GitHub PAT to install models in your projects.\n",
    "\n",
    "* **Free to start**: No charges until you hit our rate limits.\n",
    "\n",
    "Explore the full [model catalog](https://github.com/marketplace/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c9676f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32ae00e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440f826b",
   "metadata": {},
   "source": [
    "## OpenAI Client\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21a13e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(\n",
    "    base_url=\"https://models.inference.ai.azure.com\",\n",
    "    api_key=os.environ[\"GITHUB_TOKEN\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ba5b1c",
   "metadata": {},
   "source": [
    "## Chat Completions API\n",
    "\n",
    "The Chat Completions API endpoint will generate a model response from a list of messages comprising a conversation.\n",
    "\n",
    "\n",
    "|Name | Type | Description | Required | \n",
    "| --- | --- | --- | --- |\n",
    "| messages | array | A list of messages comprising the conversation so far. | Yes | \n",
    "| model | \tstring |\tA model deployment name to be used for chat completions | \tYes |  \n",
    "| tools | array | A list of tools that the model can use to generate completions. | No | \n",
    "| max_completion_tokens  | integer  | An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens. | No  |  \n",
    "| stream\t| boolean |\tWhether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message. |\tNo\t| \n",
    "\n",
    "\n",
    "These are some of the parameters you can use to customize the model's behavior. For a full list of parameters, see the [Chat Completions API documentation](https://platform.openai.com/docs/api-reference/chat).\n",
    "\n",
    "> *Note*: try the new Responses API to take advantage of the lastest OpenAI platform features. \n",
    "\n",
    "References: \n",
    "- Chat Completions API in the OpenAI SDK: https://platform.openai.com/docs/api-reference/chat\n",
    "- Chat Completions API in the Azure OpenAI inference API: https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions\n",
    "- Responses API: https://platform.openai.com/docs/api-reference/responses\n",
    "- Compare Chat Completions with Responses: https://platform.openai.com/docs/guides/responses-vs-chat-completions?api-mode=responses\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03b694b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If drying the shirts is an independent process (meaning each shirt dries on its own without interference from the others), **it still takes just 1 hour to dry all three shirts** simultaneously. The drying process is not dependent on quantity, assuming you have the space or means to dry all shirts at the same time.\n",
      "\n",
      "For example:\n",
      "- If you can hang up all three shirts together or put them in a dryer at the same time, it will take **1 hour**.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = await client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"developer\",\n",
    "            \"content\": \"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"1 shirt needs 1 hour to dry, how long does it take to dry 3 shirts\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-4o\",\n",
    "    # reasoning_effort=\"high\",\n",
    "\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158acf90",
   "metadata": {},
   "source": [
    "## Chat Completion Response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5307b7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-BGXA0z0Nmo8RfT2YDocG5Zb0QW5O1\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"message\": {\n",
      "        \"content\": \"If drying the shirts is an independent process (meaning each shirt dries on its own without interference from the others), **it still takes just 1 hour to dry all three shirts** simultaneously. The drying process is not dependent on quantity, assuming you have the space or means to dry all shirts at the same time.\\n\\nFor example:\\n- If you can hang up all three shirts together or put them in a dryer at the same time, it will take **1 hour**.\\n\",\n",
      "        \"refusal\": null,\n",
      "        \"role\": \"assistant\"\n",
      "      },\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"protected_material_code\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"protected_material_text\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1743280416,\n",
      "  \"model\": \"gpt-4o-2024-11-20\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"system_fingerprint\": \"fp_ded0d14823\",\n",
      "  \"usage\": {\n",
      "    \"completion_tokens\": 96,\n",
      "    \"prompt_tokens\": 30,\n",
      "    \"total_tokens\": 126,\n",
      "    \"completion_tokens_details\": {\n",
      "      \"accepted_prediction_tokens\": 0,\n",
      "      \"audio_tokens\": 0,\n",
      "      \"reasoning_tokens\": 0,\n",
      "      \"rejected_prediction_tokens\": 0\n",
      "    },\n",
      "    \"prompt_tokens_details\": {\n",
      "      \"audio_tokens\": 0,\n",
      "      \"cached_tokens\": 0\n",
      "    }\n",
      "  },\n",
      "  \"prompt_filter_results\": [\n",
      "    {\n",
      "      \"prompt_index\": 0,\n",
      "      \"content_filter_results\": {\n",
      "        \"hate\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"jailbreak\": {\n",
      "          \"filtered\": false,\n",
      "          \"detected\": false\n",
      "        },\n",
      "        \"self_harm\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"sexual\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        },\n",
      "        \"violence\": {\n",
      "          \"filtered\": false,\n",
      "          \"severity\": \"safe\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.to_json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9dd9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
