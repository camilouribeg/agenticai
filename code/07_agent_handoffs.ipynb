{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34246019",
   "metadata": {},
   "source": [
    "# Agent Handoffs - Learning Objectives\n",
    "\n",
    "This notebook demonstrates how to implement agent handoffs in AI applications using the OpenAI Agents SDK. By the end of this notebook, you'll understand:\n",
    "\n",
    "1. **Agent Handoff Architecture** - How to design and implement a system where multiple specialized agents collaborate on complex tasks\n",
    "   \n",
    "2. **Agent Specialization** - How to create agents with specific capabilities and responsibilities:\n",
    "   - Content generation (Dad Jokes Agent)\n",
    "   - Response formatting (User Response Writer)\n",
    "   - Task orchestration (Orchestrator Agent)\n",
    "\n",
    "3. **Workflow Orchestration** - How to design an agent that can delegate tasks to specialized agents based on user requests\n",
    "\n",
    "4. **Structured Output Handling** - How to define and use Pydantic models for structured agent responses\n",
    "\n",
    "5. **Agent Communication Patterns** - How to implement bidirectional handoffs between agents to create complete workflows\n",
    "\n",
    "6. **Workflow Visualization** - How to create visual representations of agent communication pathways\n",
    "\n",
    "7. **Interactive Conversation Management** - How to maintain conversation context through multiple agent handoffs\n",
    "\n",
    "\n",
    "This notebook guides you through designing, implementing, and testing a multi-agent system with handoffs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe52035e",
   "metadata": {},
   "source": [
    "code references: \n",
    "https://github.com/openai/openai-agents-python/blob/main/examples/basic/agent_lifecycle_example.py \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f0c45d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from agents import (\n",
    "\n",
    "    Agent,\n",
    "    Runner,\n",
    "    RunConfig,\n",
    "    trace,\n",
    "    handoff,\n",
    "    ModelSettings\n",
    ")\n",
    "\n",
    "from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n",
    "from agents import handoff, RunContextWrapper\n",
    "\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467e7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, RawResponsesStreamEvent, Runner, TResponseInputItem, trace\n",
    "from helpers.trace_util import get_trace_url\n",
    "from agents.extensions.visualization import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c00e1ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.model_client import (\n",
    "    get_openai_client,\n",
    "    get_github_model_provider\n",
    ")\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "GITHUB_MODEL_PROVIDER = get_github_model_provider(\n",
    "    client = get_openai_client(),\n",
    "    model = model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd657336",
   "metadata": {},
   "source": [
    "# Define the Agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645a671",
   "metadata": {},
   "source": [
    "## Author Agent \n",
    "\n",
    "Dad Jokes agent generates new content. It procduces a structured output as instance of class `Joke`\n",
    "\n",
    "Notice the instrutions and handoff description prompts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1c7ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str = Field(description=\"The joke text\")\n",
    "    punchline: str = Field(description=\"The punchline of the joke\")\n",
    "    language: str = Field(description=\"The language of the joke\")\n",
    "\n",
    "dad_jokes_agent = Agent(\n",
    "    name=\"data_jokes_author\",\n",
    "    instructions=(\n",
    "        # f\"{RECOMMENDED_PROMPT_PREFIX} \\n\"\n",
    "        \"You are an agent that specialises in creating jokes. \"\n",
    "        \"You do not answer any other questions. \"\n",
    "        \"always deliver a punchline. \"\n",
    "        \" ## IMPORTANT ##\"\n",
    "        \"Always respond in English, even if the user asks in a different language. \"\n",
    "        \"you generate structured output that is not suitable for end user consumption. \"\n",
    "    ),\n",
    "    handoff_description=(\n",
    "        \"An agent capable of creating new jokes based on user input \"\n",
    "    ),\n",
    "    model=model_name,\n",
    "    output_type=Joke,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f544058",
   "metadata": {},
   "source": [
    "## Response Writer agent\n",
    "\n",
    "The response writer is responsible for producing a final response to the user. It takes the output of full execution context and turn it into a user-friendly message. \n",
    "\n",
    "\n",
    "In this case, behinging handed the full context is important. Handoffs will hand over the control with the full message history, which the writer in this case will need in order to produce the final response. \n",
    "\n",
    "\n",
    "The dad jokes generator agent didnot necessarly need to be aware of the full context, it could hav been implemented using the as_tool pattern as all it needed is the topic and language of the content to generate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba403cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_response_agent = Agent[Joke](\n",
    "    name=\"user_response_writer\",\n",
    "    instructions=(\n",
    "        # f\"{RECOMMENDED_PROMPT_PREFIX} \\n\"\n",
    "        \"You are an agent that writes messaages to be presented to end users.\"\n",
    "        \"your response always include summary of steps taken to produce the final output  \"\n",
    "        \"in bullet points format.\"\n",
    "        \"Output must be formatted as markdown.\"\n",
    "    ),\n",
    "    handoff_description=\"An agent capable of generating final user responses from any input provided in a presentable format.\",\n",
    "    output_type=str,\n",
    "    model=model_name,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b6f34f",
   "metadata": {},
   "source": [
    "## Orchestrator Agent\n",
    "\n",
    "\n",
    "The orchestrator in this case is always handing off to one of the two agents to either generate content to produce a formatted user response.\n",
    "\n",
    "The dad jokes agent handoffs was also updated to handdoff control back to the orchestrator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f9f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orchestrator_agent = Agent[Joke](\n",
    "    name=\"orchestrator_agent\",\n",
    "    model=model_name,\n",
    "    tool_use_behavior=\"run_llm_again\",\n",
    "    instructions=(\n",
    "        # f\"{RECOMMENDED_PROMPT_PREFIX} \\n\"\n",
    "        \"You are an orchestrator agent. \"\n",
    "        \"You think about the user request, create a plan to solve the task, \"\n",
    "        \"You have the options to pass the task to content generation agents, or \" \\\n",
    "        \"to user response writers, each specialise in their domain. \"\n",
    "        \"You identiy the next agent to carry out the task then handoff to them\"\n",
    "        \"You are not allowed to end the conversation directly, \"\n",
    "        \"only user response writers are allowed to complete the conversation.\"\n",
    "    ),\n",
    "    handoff_description=(\n",
    "        \"An agent that orchestrates the task by defining an \"\n",
    "        \"execution plan based on available agents and tools and inputs,\"\n",
    "        \"then  handing off to other agents. \"\n",
    "        ),\n",
    "    handoffs=[dad_jokes_agent,user_response_agent],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fba404b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dad_jokes_agent.handoffs = [handoff(\n",
    "    orchestrator_agent,\n",
    "    tool_description_override=\"\"\"\n",
    "        An agent capable of routing the output to the correct\n",
    "        next agent for packaing the response for delivery\n",
    "        \"\"\"\n",
    "        )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4f940",
   "metadata": {},
   "source": [
    "## Visualising the the Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86f0a40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: failed to execute WindowsPath('dot'), make sure the Graphviz executables are on your systems' PATH\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gv = draw_graph(\n",
    "        orchestrator_agent,\n",
    "        filename=\"./viz/07_agent_handoffs_orchestrator_agent_v2.gv\",\n",
    "\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646b638",
   "metadata": {},
   "source": [
    "# Implementing Agent Handoffs with the Agents SDK\n",
    "\n",
    "![handoff flow](./viz/07_agent_handoffsv2.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "412fe1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace URL: https://platform.openai.com/traces/trace?trace_id=trace_88c1cdbae56c482396a68de3998a65a1\n",
      "Why did the chicken get into quantum computing?\n",
      "\n",
      "Because it wanted to be in a superposition of both crossing the road and not crossing the road at the same time!\n",
      "\n",
      "---\n",
      "\n",
      "### Steps Taken:\n",
      "1. **Identified the Joke Theme**: Combined chickens and quantum computing.\n",
      "2. **Incorporated Quantum Concept**: Used the idea of \"superposition\" from quantum mechanics.\n",
      "3. **Added Humor**: Played on the classic \"why did the chicken cross the road?\" joke.\n",
      "4. **Finalized the Joke**: Delivered a lighthearted punchline.\n",
      "Agent(name='user_response_writer', instructions='You are an agent that writes messaages to be presented to end users.your response always include summary of steps taken to produce the final output  in bullet points format.Output must be formatted as markdown.', handoff_description='An agent capable of generating final user responses from any input provided in a presentable format.', handoffs=[], model='gpt-4o', model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=<class 'str'>, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True)\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, RawResponsesStreamEvent, Runner, TResponseInputItem, trace\n",
    "\n",
    "msg = input (\"Enter your message: \") or \"Hello, tell me a joke about Chickens and Quantum Computing\"\n",
    "\n",
    "inputs: list[TResponseInputItem] = [{\"content\": msg, \"role\": \"user\"}]\n",
    "\n",
    "result = None\n",
    "try:\n",
    "    with trace(workflow_name=\"handoffs\", group_id=\"mdsi\") as tr:\n",
    "        print(f\"Trace URL: {get_trace_url(tr)}\")\n",
    "        result = await Runner.run(\n",
    "            starting_agent=orchestrator_agent,\n",
    "            input=inputs,\n",
    "            run_config=RunConfig(\n",
    "                model=model_name,\n",
    "                model_provider=GITHUB_MODEL_PROVIDER,\n",
    "                model_settings=ModelSettings(\n",
    "                    **{\"temperature\": 0.2, \"max_tokens\": 5000}\n",
    "                )),\n",
    "            max_turns=5,\n",
    "        )\n",
    "\n",
    "        print(result.final_output)\n",
    "        print(result.last_agent)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546884fa",
   "metadata": {},
   "source": [
    "## User Interaction \n",
    "\n",
    "Run the agent in a streemed output mode and interact with user to carry out a conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa0dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.responses import ResponseContentPartDoneEvent, ResponseTextDeltaEvent\n",
    "\n",
    "from agents import Agent, RawResponsesStreamEvent, Runner, TResponseInputItem, trace\n",
    "import uuid\n",
    "\n",
    "conversation_id = str(uuid.uuid4().hex[:16])\n",
    "\n",
    "msg = input(\"Hi! We speak French, Spanish and English. How can I help? \")\n",
    "agent = orchestrator_agent\n",
    "inputs: list[TResponseInputItem] = []\n",
    "inputs.append({\"content\": msg, \"role\": \"user\"})\n",
    "\n",
    "while True:\n",
    "    # Each conversation turn is a single trace. Normally, each input from the user would be an\n",
    "    # API request to your app, and you can wrap the request in a trace()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    with trace(\"Routing example\", group_id=conversation_id) as tr:\n",
    "        print(f\"Trace URL: {get_trace_url(tr)}\")\n",
    "        result = Runner.run_streamed(\n",
    "            agent,\n",
    "            input=inputs,\n",
    "            run_config=RunConfig(model=model_name, model_provider=GITHUB_MODEL_PROVIDER),\n",
    "        )\n",
    "        async for event in result.stream_events():\n",
    "            if not isinstance(event, RawResponsesStreamEvent):\n",
    "                continue\n",
    "            data = event.data\n",
    "            if isinstance(data, ResponseTextDeltaEvent):\n",
    "                print(data.delta, end=\"\", flush=True)\n",
    "            elif isinstance(data, ResponseContentPartDoneEvent):\n",
    "                print(\"\\n\")\n",
    "\n",
    "    inputs = result.to_input_list()\n",
    "    print(\"\\n\")\n",
    "\n",
    "\n",
    "    agent = result.current_agent\n",
    "    user_msg = input(\"Enter a message: \")\n",
    "    if user_msg in (\"exit\", \"quit\", \"\"):\n",
    "        break\n",
    "    inputs.append({\"content\": user_msg, \"role\": \"user\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
